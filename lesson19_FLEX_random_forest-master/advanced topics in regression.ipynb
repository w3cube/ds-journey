{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["#### Learning Objectives\n", "By the end of this lesson, you will be able to..\n", "\n", "- Create interaction terms.\n", "- Recognize when a log transformation is appropriate.\n", "- Use regularization to reduce overfitting.\n", "- Distinguish between time-series and cross-sectional data.\n", "- Describe time-series data in terms of trend, seasonality, and noise.\n", "- Explain how train/test splits need to be performed differently for time-series data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Interaction Terms"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Advanced Topics in Regression"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Using Transformed Variables in Regression Models"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Linear regression creates a model that is linear in the features that you pass into it."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import pandas as pd\n", "import seaborn as sns\n", "\n", "%matplotlib inline"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mammals_path = Path('.', 'data', 'mammals.txt')\n", "cols = ['brain','body']\n", "mammals = pd.read_csv(mammals_path, sep='\\t', names=cols, header=0)\n", "mammals = mammals.loc[mammals.loc[:, 'body'] < 200, :].sort_values('body')\n", "mammals.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\n", "\n", "X = mammals.loc[:, ['body']]\n", "y = mammals.loc[:, 'brain']\n", "\n", "linreg = LinearRegression()\n", "linreg.fit(X, y)\n", "y_fit = linreg.predict(X)\n", "plt.plot(X.values, y_fit)\n", "plt.scatter(X.values, y);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["But it can capture non-linear relationships with your original features if you give it non-linear transformations of those features."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# Re-run the regression with an additional squared term\n", "mammals.loc[:, 'body_squared'] = mammals.loc[:, 'body']**2\n", "\n", "X = mammals.loc[:, ['body', 'body_squared']]\n", "y = mammals.loc[:, 'brain']\n", "\n", "linreg = LinearRegression()\n", "linreg.fit(X, y)\n", "y_fit = linreg.predict(X)\n", "\n", "plt.plot(mammals.loc[:, 'body'].values, y_fit)\n", "plt.scatter(mammals.loc[:, 'body'], y);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Polynomial Terms"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A polynomial function of x has the form $c_0 + c_1x + c_2x^2 + c_3x^3 + \\ldots$.\n", "\n", "If you give a linear regression model $x$, $x^2$, and $x^3$ as features, for instance, it will find the $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\beta_3$ that minimizes mean-squared error for using $\\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3$ to predict $y$.\n", "\n", "It can always recover simple linear regression by setting the coefficients on the higher-order terms to 0, so adding these higher-order terms only increases the set of relationships that the model can capture."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise.**\n", "\n", "- How does adding higher-order polynomial terms as inputs to a linear regression model affect its bias and variance?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Decreases (or at least does not increase) bias, increases variance."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Every additional polynomial term gives your model an additional chance to change directions.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# first-order\n", "x = np.linspace(-1, 1, 100)\n", "plt.plot(x, x);"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# second-order\n", "plt.plot(x, x**2);"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# third-order\n", "x = np.linspace(-.75, 1.5, 100)\n", "plt.plot(x, -.4*x-x**2+x**3);"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# fourth-order\n", "x = np.linspace(-1, 1, 100)\n", "plt.plot(x, -x**2+x**4);"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Too many polynomial terms leads to overfitting\n", "fig = sns.lmplot(x='body', y='brain', data=mammals, ci=None, order=8);"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# An (n-1)-order polynomial can always fit n data points perfectly.\n", "# Definitely overfitting!\n", "fig = sns.lmplot(x='body', y='brain', data=mammals, ci=None, order=50);\n", "ax = fig.axes\n", "ax[0,0].set_ylim(0, 200);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Including multiple transformations of one variable complicates coefficient interpretation.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mammals.loc[:, 'body_squared'] = mammals.loc[:, 'body']**2\n", "\n", "X = mammals.loc[:, ['body', 'body_squared']]\n", "y = mammals.loc[:, 'brain']\n", "\n", "linreg = LinearRegression()\n", "linreg.fit(X, y)\n", "y_fit = linreg.predict(X)\n", "\n", "plt.plot(mammals.loc[:, 'body'].values, y_fit)\n", "plt.scatter(mammals.loc[:, 'body'], y);"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(linreg.intercept_)\n", "print(linreg.coef_)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Write down the fitted model we just created."]}, {"cell_type": "markdown", "metadata": {}, "source": ["$brain = .132 + .108*body + .0021 * body^2$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- How would you normally interpret the coefficient on `body` in this model? Why doesn't that interpretation work in this case?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You would normally interpret the coefficient on a variable in a linear regression model as telling you how your model's predictions change with a one-unit increase in that variable, holding all other variables fixed. That interpretation doesn't work here because you can't have a one-unit increase in `body` while holding `body^2` fixed."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**sklearn has a \"transformer\" that generates polynomial terms**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# sklearn transformers have the same interface as \"estimators\" (models)\n", "# except that you fit them on features and use them to transform features,\n", "# rather than fitting them on features and a target and using them to predict\n", "# target values.\n", "from sklearn.preprocessing import PolynomialFeatures\n", "\n", "X = mammals.loc[:, ['body']]\n", "pf = PolynomialFeatures(degree=3, include_bias=False)\n", "pf.fit(X)\n", "pf.transform(X)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use the Boston housing data for the exercises below."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["from sklearn.datasets import load_boston\n", "\n", "boston = load_boston()\n", "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n", "y = pd.DataFrame(boston.target, columns=['MEDV'])\n", "boston = pd.concat([X, y], axis=1)\n", "boston.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Create a linear regression model for MEDV against DIS with no higher-order polynomial terms."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = boston.loc[:, ['DIS']]\n", "y = boston.loc[:, 'MEDV']\n", "\n", "lr_boston1 = LinearRegression()\n", "lr_boston1.fit(X, y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Create a linear regression model for y against X polynomial terms up to and including degree seven."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pf = PolynomialFeatures(degree=7, include_bias=False)\n", "pf.fit(X)\n", "X7 = pf.transform(X)\n", "\n", "lr_boston7 = LinearRegression()\n", "lr_boston7.fit(X7, y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Use 5-fold cross-validation to choose the polynomial order between 1 and 10 that gives the best results in terms of MSE on held-out data. *Hint*: use `sklearn.model_selection.cross_val_score`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import cross_val_score, KFold\n", "\n", "kf = KFold(n_splits=5, shuffle=True)\n", "\n", "for poly_degree in range(1, 11):\n", "    pf = PolynomialFeatures(degree=poly_degree, include_bias=False)\n", "    X_poly = pf.fit_transform(X)\n", "    lr = LinearRegression()\n", "    score = np.mean(-cross_val_score(lr, X_poly, y, cv=kf, scoring='neg_mean_squared_error'))\n", "    print(poly_degree, score)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- **Bonus:** Create a model with $DIS$ and $DIS^{-1}$ as features and score it using 5-fold cross-validation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["kf = KFold(n_splits=5, shuffle=True)\n", "\n", "X_poly = pd.DataFrame(boston.loc[:, ['DIS']])\n", "X_poly.loc[:, '1/DIS'] = X.loc[:, 'DIS']**(-1)\n", "lr = LinearRegression()\n", "np.mean(-cross_val_score(lr, X_poly, y, cv=kf, scoring='neg_mean_squared_error'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- **Bonus:** Create line plots of your models' fitted values as a function of DIS and overlay them on scatterplots of MEDV against DIS."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["y_pred_boston1 = lr_boston1.predict(X)\n", "\n", "plt.plot(X.values, y_pred_boston1, color='red')\n", "plt.scatter(boston.loc[:, 'DIS'], boston.loc[:, 'MEDV'], alpha=.2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pf = PolynomialFeatures(degree=3)\n", "X3 = pf.fit_transform(X)\n", "\n", "lr_boston3 = LinearRegression()\n", "lr_boston3.fit(X3, y)\n", "\n", "y_pred_boston3 = lr_boston3.predict(X3)\n", "\n", "sorted_boston = pd.DataFrame(boston.loc[:, 'DIS'])\n", "sorted_boston.loc[:, 'pred3'] = y_pred_boston3\n", "sorted_boston.sort_values('DIS', inplace=True)\n", "\n", "plt.plot(sorted_boston.loc[:, 'DIS'], sorted_boston.loc[:, 'pred3'], color='red')\n", "plt.scatter(boston.loc[:, 'DIS'], boston.loc[:, 'MEDV'], alpha=.2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["y_pred_boston7 = lr_boston7.predict(X7)\n", "\n", "sorted_boston = pd.DataFrame(boston.loc[:, 'DIS'])\n", "sorted_boston.loc[:, 'pred7'] = y_pred_boston7\n", "sorted_boston.sort_values('DIS', inplace=True)\n", "\n", "plt.plot(sorted_boston.loc[:, 'DIS'], sorted_boston.loc[:, 'pred7'], color='red')\n", "plt.scatter(boston.loc[:, 'DIS'], boston.loc[:, 'MEDV'], alpha=.2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_neg = pd.DataFrame(boston.loc[:, ['DIS']])\n", "X_neg.loc[:, '1/DIS'] = X.loc[:, 'DIS']**(-1)\n", "\n", "lr_boston_neg = LinearRegression()\n", "lr_boston_neg.fit(X_neg, y)\n", "\n", "y_pred_boston_neg = lr_boston_neg.predict(X_neg)\n", "\n", "sorted_boston = pd.DataFrame(boston.loc[:, 'DIS'])\n", "sorted_boston.loc[:, 'pred_neg'] = y_pred_boston_neg\n", "sorted_boston.sort_values('DIS', inplace=True)\n", "\n", "plt.plot(sorted_boston.loc[:, 'DIS'], sorted_boston.loc[:, 'pred_neg'], color='red')\n", "plt.scatter(boston.loc[:, 'DIS'], boston.loc[:, 'MEDV'], alpha=.2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Notes.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- In statistics, it is extremely unusual to use more than a third-order polynomial.\n", "- Higher-order polynomials are more common in machine learning, where the emphasis is on predictive accuracy rather than understanding.\n", "- It would be unusual to use a polynomial term without including all lower-order polynomial terms.\n", "- In addition to polynomial terms (with positive integer exponents), it can also be beneficial to include terms with negative exponents (e.g. $x^{-1}=1/x$) and/or fractional exponents (e.g. $x^{1/2}=\\sqrt{X}$)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Interaction Terms"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Sometimes the significance of one feature depends on the value of another feature.\n", "\n", "For instance, perhaps median housing prices increase as you get closer to a major employment center *unless crime is high around that area*."]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can model these kinds of \"interaction effects\" by including the *products* of the interacting variables as features in our models.\n", "\n", "For example:\n", "\n", "$$MEDV = \\beta_0 + \\beta_1 * DIS + \\beta_2 * CRIM + \\beta_{12} (DIS * CRIM)$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Implement the model above\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise.** Write down the fitted model we just created."]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# check descriptive stats\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Recall the usual interpretation of the coefficient on DIS:** how much the model's prediction for MEDV changes with a one-unit increase in DIS, all else being equal (i.e. for a particular value of CRIM).\n", "\n", "**With interaction terms, interpreting the coefficients for a feature DIS requires specifying particular values for the interacting variables.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For instance, if CRIM is fixed at its 25th percentile value of 0.082, we get\n", "\n", "$MEDV = 22.62 + 0.48 * DIS + 0.467 * CRIM - 0.527 (DIS * CRIM)$\n", "\n", "$MEDV = 22.62 + 0.48 * DIS + 0.467 * 0.082 - 0.527 (DIS * 0.082)$\n", "\n", "$MEDV = 22.62 + 0.48 * DIS + 0.038 - 0.043 * DIS$\n", "\n", "$MEDV = 22.658 + 0.437 * DIS$\n", "\n", "So **at CRIM=.082**, the model's prediction for MEDV increases by .437 when DIS increases by one. It's better on average to be close to employment centers when crime is low.\n", "\n", "The story is different when CRIM has its 75th percentile value of 3.64:\n", "\n", "$MEDV = 22.62 + 0.48 * DIS + 0.467 * CRIM - 0.527 (DIS * CRIM)$\n", "\n", "$MEDV = 22.62 + 0.48 * DIS + 0.467 * 3.64 - 0.527 (DIS * 3.64)$\n", "\n", "$MEDV = 22.62 + 0.48 * DIS + 1.70 - 1.92 * DIS$\n", "\n", "$MEDV = 24.32 -1.44 * DIS$\n", "\n", "**At CRIM=3.64**, the model's prediction for MEDV *decreases* by 1.44 when DIS increases by one. It's better on average to be farther from employment centers when crime is high."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- How does adding interaction terms affect a model's bias and variance?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Using 5-fold cross-validation, calculate the MSE for a model predicting MEDV from DIS and CRIM without an interaction term. *Hint*: use sklearn.model_selection.cross_val_score"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Using 5-fold cross-validation, calculate the MSE for a model predicting MEDV from DIS and CRIM with an interaction term. *Hint*: use sklearn.model_selection.cross_val_score"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Log Transformations"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When your data is very skewed, try a log transformation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mammals = (\n", "    pd.read_csv(mammals_path, sep='\\t', names=cols, header=0)\n", "    .dropna()\n", "    .sort_values('body')\n", "    .reset_index(drop=True)\n", ")\n", "mammals = mammals.loc[mammals.loc[:, 'body'] < 200, :]\n", "mammals.hist();\n", "mammals.plot(kind='scatter', x='body', y='brain');"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["log_mammals = np.log(mammals)\n", "log_mammals.hist()\n", "log_mammals.plot(kind='scatter', x='body', y='brain')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Because we applied a log transformation to $y$ as well as $x$, we need to be careful about how we interpret the MSE values."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# Train and score a linear model in the original space.\n", "# This model isn't overfitting significantly, so let's not\n", "# worry about a train/test split.\n", "from sklearn import metrics\n", "\n", "X = mammals.loc[:, ['body']]\n", "y = mammals.loc[:, 'brain']\n", "lr_mammals = LinearRegression()\n", "lr_mammals.fit(X, y)\n", "y_pred = lr_mammals.predict(X)\n", "metrics.mean_squared_error(y, y_pred)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Train and score a linear model in the log-transformed space.\n", "# This model isn't overfitting significantly, so let's not\n", "# worry about a train/test split.\n", "log_mammals = np.log(mammals)\n", "X_log = log_mammals.loc[:, ['body']]\n", "y_log = log_mammals.loc[:, 'brain']\n", "\n", "lr_log_mammals = LinearRegression()\n", "lr_log_mammals.fit(X_log, y_log)\n", "y_pred_log = lr_log_mammals.predict(X_log)\n", "metrics.mean_squared_error(y_log, y_pred_log)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Not a fair comparison! MSE for the second model is in log-space."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["# Get MSE for the log-log model in the original space\n", "metrics.mean_squared_error(y, np.exp(y_pred_log))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**What's going on?**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["plt.scatter(X.values, y);\n", "plt.plot(X, np.exp(y_pred_log));"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.scatter(X.values, y);\n", "plt.plot(X, y_pred);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The model that we fit in log-log space is getting killed by the points in the top-right:\n", "\n", "- MSE punishes large errors.\n", "- Errors that are large in the original space don't look so large in log-log space, so the model doesn't focus on them as much as it \"should.\"\n", "\n", "The log-log model is still better in at least two respects:\n", "\n", "- It conveys more understanding: modeling log of brain size as a linear function of log of body size plus random noise seems to capture what is really going on.\n", "- It makes better predictions in terms of MAE."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Calculate MAE for the original and log-log models\n", "print(metrics.mean_absolute_error(y, np.exp(y_pred_log)))\n", "print(metrics.mean_absolute_error(y, y_pred))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Notes**\n", "\n", "- A log-transformed variable typically replaces the original variable in a regression analysis, unlike a polynomial term.\n", "- You can apply a log transformation to any combination of your features and your target variable."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Summary\n", "\n", "- Linear regression *can* capture non-linear relationships *when you provide the appropriate non-linear transformations*.\n", "- Every polynomial term you add allows your model to change directions once.\n", "- Log transformations are awesome for skewed variables."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Regularization"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"bonus-material-regularization\"></a>\n", "### Why Regularize?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Recall this overly complicated model:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Too many polynomial terms leads to overfitting\n", "fig = sns.lmplot(x='body', y='brain', data=mammals, ci=None, order=8);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["One way to make this model behave in a more reasonable way is to reduce the number of features we give it.\n", "\n", "Another way is to *make it pay to use those features*. This approach is called **regularization**."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"how-does-regularization-work\"></a>\n", "### How Does Regularization Work?\n", "\n", "For a normal linear regression model, we estimate the coefficients using the least squares criterion, which minimizes the residual sum of squares (RSS)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["For a regularized linear regression model, we minimize the sum of RSS and a \"penalty term\" that penalizes coefficient size.\n", "\n", "**Ridge regression** (or \"L2 regularization\") minimizes: $$\\text{RSS} + \\alpha \\sum_{j=1}^p \\beta_j^2$$\n", "\n", "**Lasso regression** (or \"L1 regularization\") minimizes: $$\\text{RSS} + \\alpha \\sum_{j=1}^p |\\beta_j|$$\n", "\n", "- $p$ is the number of features.\n", "- $\\beta_j$ is a model coefficient.\n", "- $\\alpha$ is a tuning parameter:\n", "    - A tiny $\\alpha$ imposes no penalty on the coefficient size, and is equivalent to a normal linear regression model.\n", "    - Increasing the $\\alpha$ penalizes the coefficients and thus shrinks them."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"lasso-and-ridge-path-diagrams\"></a>\n", "### Lasso and Ridge Path Diagrams\n", "\n", "A larger alpha (toward the left of each diagram) results in more regularization:\n", "\n", "- Lasso regression shrinks coefficients all the way to zero, thus removing them from the model.\n", "- Ridge regression shrinks coefficients toward zero, but they essentially never reach zero.\n", "\n", "Source code for the diagrams: [Lasso regression](http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html) and [Ridge regression](http://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![Lasso and Ridge Coefficient Plots](./assets/lasso_ridge_path.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"advice-for-applying-regularization\"></a>\n", "### Advice for Applying Regularization\n", "\n", "**Features should be standardized** so that the penalty is not sensitive to the scale of the variables.\n", "\n", "**How should you choose between lasso regression and ridge regression?**\n", "\n", "- Lasso regression is preferred if we believe many features are irrelevant or if we prefer a sparse model.\n", "- Ridge can work particularly well if there is a high degree of colinearity in your model.\n", "- If model performance is your primary concern, it is best to try both.\n", "- Elastic net regression is a combination of lasso regression and ridge regression."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"ridge-regression\"></a>\n", "### Example\n", "\n", "- [Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) documentation\n", "- **alpha:** must be positive, increase for more regularization\n", "- **normalize:** scales the features (without using StandardScaler)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["bikes_path = Path('.', 'data', 'bikeshare.csv')\n", "bikes = pd.read_csv(bikes_path, index_col='datetime', parse_dates=True)\n", "bikes_dummies = pd.get_dummies(bikes, columns=['season']).drop('season_1', axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Include dummy variables for season in the model.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fit a Ridge Regression model with alpha=0 (equivalent to linear regression)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Coefficients for a non-regularized linear regression\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Be careful in interpreting these coefficients, because they are related to the *normalized* versions of the input variables."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Try alpha=0.1.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Examine the coefficients.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice that the model tries to spread the coefficients more evenly because the squared term particularly punishes large deviations from zero."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Try Lasso with alpha=0.1\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Examine the coefficients.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Time Series Data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## What Is Time Series Data?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Cross-Sectional Data**\n", "\n", "<img src=\"./assets/cross_sectional_data.png\" width=400></img>\n", "\n", "Data is collected at a single point in time for each individual.\n", "\n", "**Time-Series/Longitudinal Data**\n", "\n", "<img src=\"./assets/time_series_data.png\" width=400></img>\n", "\n", "Data for the same variables is collected over time for the same individuals."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise.**\n", "\n", "/poll \"Which of the data sets that we have worked with so far contain time series data? (Select as many as apply.)\" \"bikes\" \"grad school admissions\" \"titanic\" \"ufo\" \"drinks\" \"boston housing\" \"nba\" \"mammals\""]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Trends, Seasonality, and Noise"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Google searches for \"data science\" show a strong increasing trend over the last five years:\n", "\n", "![](./assets/data_science.png)\n", "\n", "Google searches for \"gingerbread house\" show strong annual seasonality:\n", "\n", "![](./assets/gingerbread_house.png)\n", "\n", "Google searches for \"iphone\" show both trend and seasonality:\n", "\n", "![](./assets/iphone.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise.**\n", "\n", "Go to https://trends.google.com/trends/ and search for a term that interests you. Post it on Slack with a sentence or two about any trend or seasonality that it exhibits."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train/Test Split with Time Series Data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["All training data come from before all test data -- your model should not be peeking into the future!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Practice"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Work on the practice notebooks from Unit 3 in pairs using the driver/navigator approach. One person (the \"driver\") writes the code (sharing his or her screen) while the other person (the \"navigator\") continually makes suggestions and reviews the code. The driver should talk about what he or she is doing, ask for input, and generally keep the navigator engaged.\n", "\n", "We will switch driver/navigator roles when are time is halfway up.\n", "\n", "Pick a notebook you haven't worked on yet if possible, especially when you are the driver. If you are the navigator for a notebook you have already worked on, don't pull out your code. Let your partner take the lead and give feedback on the direction they take."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Projects\n", "\n", "- Final Project Pt 3 due today\n", "- Unit Project 3 (optional) due next Thurs."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Questions?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["```\n", "=========================================\n", "@channel\n", "Exit Ticket: https://goo.gl/forms/OUw4gyTiRKMOTI3t2        \n", "\n", "#feedback\n", "=========================================\n", "```"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.5"}}, "nbformat": 4, "nbformat_minor": 1}