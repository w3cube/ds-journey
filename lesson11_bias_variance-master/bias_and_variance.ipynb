{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Train/Test Split and Bias and Variance\n",
    "\n",
    "_Authors: Joseph Nelson (DC), Kevin Markham (DC)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "<a id=\"learning-objectives\"></a>\n",
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Characterize the difference between error due to bias and error due to variance.\n",
    "- Identify the bias-variance trade-off.\n",
    "- Describe what overfitting and underfitting means in the context of model building.\n",
    "- Explain problems associated with over- and underfitting.\n",
    "- Grasp why train/test split is necessary.\n",
    "- Explore k-folds, LOOCV, and three split methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression with one input feature finds the *line* that best fits the data. This is what you want if in fact $y = \\beta_0 + \\beta_1 * x + \\epsilon$, where $\\epsilon$ is normally distributed random noise.\n",
    "\n",
    "![](./assets/linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if, say, $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 +\\epsilon$, then \"the line that best fits the data\" is *not* what you want.\n",
    "\n",
    "![](./assets/quadratic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This difference between the kind of relationship a model can capture and the kind that exists in the underlying data-generating process (the \"true model\") is called **bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is due to a fundamentally mis-specified model, *bias doesn't go away as you collect more data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to address bias is to make your model more flexible. For instance, a model that finds the best-fitting parabola $y = \\beta_0 + \\beta_1 X + \\beta_2 X^2$ can do everything a linear model can do (setting $\\beta_2=0$) and more, so it can only be better in terms of bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why stop there? Why not $y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\beta_4 X^4 + \\beta_5 X^5 + \\beta_6 X^6 + \\beta_7 X^7 + \\beta_8 X^8$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem why this approach is that more flexible models are more sensitive to that random noise $\\epsilon$ in the data. They find \"patterns\" that aren't really there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/overfit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because they are sensitive to random noise in the data, overly flexible models will give very different results on datasets that are drawn from the same distribution.\n",
    "\n",
    "![](./assets/overfit_side_by_side.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this reason, we say that overly flexible models have high **variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As sample size increases, even a very flexible model will learn to distinguish between signal and noise. As a result, *model variance does go away as you collect more data*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bias-and-variance-trade-off\"></a>\n",
    "## Bias-Variance Tradeoff\n",
    "\n",
    "**Exercise.** State whether each of the following changes you can make to a model would increase, decrease, or leave unchanged (1) model bias and (2) model variance, or whether its effect on those quantities cannot be determined from the information given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Include more of the variables that are given in your dataframe in your feature columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- decrease model bias\n",
    "-- variance increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add the square of one of your feature columns as a new feature column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- decrease model bias\n",
    "-- variance increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replace one feature column with its logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "depends which version of the variable is better fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collect more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- decreases variance\n",
    "-- bias remains unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modify the procedure you use to fit your model so that instead of minimizing the sum of squared errors, it minimizes the sum of squared errors plus a penalty term that increases with the coefficients of the model. (This approach is an example of *regularization*.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- u r making ur model less flexible\n",
    "-- bias goes up\n",
    "-- variance goes down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of linear regression, use a *generative additive model* that can fit a variety of non-linear relationships between your features and your target variable as well as linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- made our mode more flexible\n",
    "-- mean decreasing bias\n",
    "-- increasing variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressing bias and variance mathematically\n",
    "\n",
    "Assume that there is some function $f$ such that $y=f(X)+\\epsilon$ for a given input feature matrix $X$ and target variable $y$, where $\\epsilon$ is normally distributed with variance $\\sigma^2$. We will call this $f$ the \"true model\" for $y$ based on $X$. We will use $\\hat{f}(X)$ (\"$f$-hat of $X$\") to denote a particular model's predictions for $y$ given an input feature matrix $X$.\n",
    "\n",
    "Suppose you performed the following steps repeatedly: \n",
    "\n",
    "1. Draw a random sample of a given size from a particular data-generating process -- the \"training set.\"\n",
    "2. Fit a particular kind of model to that training set.\n",
    "3. Draw one additional point $X^*$ from the same data-generating process and calculate the squared error of the model's prediction on that point.\n",
    "\n",
    "The expected (long-run average) value of that squared error can be broken down into three components:\n",
    "\n",
    "$\\mbox{Expected Squared Error} = \\mbox{Bias}^2 + \\mbox{Variance} + \\mbox{Irreducible Error}$\n",
    "\n",
    "where\n",
    "- $\\mbox{Expected Squared Error} = E[(y-\\hat{f}(X^*))^2]$\n",
    "    - (expected squared difference between the target value and the model's prediction)\n",
    "- $\\mbox{Bias} = E[\\hat{f}(X^*) - f(X^*)]$\n",
    "    - (expected difference between a prediction from the model and a prediction from the true model)\n",
    "- $\\mbox{Variance} = E[(\\hat{f}(X^*) - E[\\hat{f}(X^*)])^2]$\n",
    "    - (expected squared difference between the model's prediction and its expected prediction)\n",
    "- $\\mbox{Irreducible Error} = \\sigma^2$\n",
    "    - (noise in the data-generating mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you build a linear model to predict the weights of golden retrievers when they are one year old based on their weights when they are seven weeks old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bias:** Suppose that the relationship between seven-week weight and one-year weight has a bit of an S-shape, because very large and very small puppies tend to regress toward the mean. *The mismatch between the true s-shaped model and the assumed linear model constitutes bias.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance:** Because your sample size is finite, each time you train the model on new data you don't quite nail even the linear relationship that's closest to the true model. *The random departures of your model from this best possibility given your bias constitute variance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Irreducible error:** Some 10-pound puppies will grow up to be 70 pounds, while others will grow up to be 90 pounds. In general, one-year weight is not perfectly predictable from seven-week weight. As a  result, *even the true model will have some irreducible error.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Would a \"true model\" $f(X)$ make perfect predictions? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Challenge:** How could you create a model for predicting some $y$ that does even better than the true model for $y$ based on your input features $X$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exploring-the-bias-variance-tradeoff\"></a>\n",
    "### Exploring the Bias-Variance Trade-Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Allow plots to appear in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"brain-and-body-weight-mammal-dataset\"></a>\n",
    "### Brain and Body Weight Mammal Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a [data set](http://people.sc.fsu.edu/~jburkardt/datasets/regression/x01.txt) of the average weight of the body (in kg) and the brain (in g) for 62 mammal species. We'll use this dataset to investigate bias vs. variance. Let's read it into Pandas and take a quick look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = os.path.join('..', 'data', 'mammals.txt')\n",
    "cols = ['brain','body']\n",
    "mammals = pd.read_csv(path, sep='\\t', names=cols, header=0)\n",
    "mammals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to focus on a smaller subset in which the body weight is less than 200 kg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Only keep rows in which the body weight is less than 200 kg.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a scatterplot (using [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/)) to visualize the relationship between brain and body weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a relationship between brain and body weight for mammals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"making-a-prediction\"></a>\n",
    "### Making a Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear-regression-quick-review\"></a>\n",
    "#### Linear Regression: A Quick Review\n",
    "\n",
    "<img src=\"./assets/linear-residuals.png\" width=800 /img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pretend that a **new mammal species** is discovered. We measure the body weight of every member of this species we can find and calculate an **average body weight of 100 kgs**. We want to **predict the average brain weight** of this species (rather than measuring it directly). How might we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drew a straight line that appears to best capture the relationship between brain and body weight. So, we might predict that our new species has a brain weight of about 45 g, as that's the approximate y value when x=100.\n",
    "\n",
    "This is known as a \"linear model\" or a \"linear regression model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"making-a-prediction-from-a-sample\"></a>\n",
    "## Making a Prediction From a Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of supervised learning is to generalize from the samples we have to samples we don't have. Let's simulate this situation by assigning each of the 51 observations to **either universe 1 or universe 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility.\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Randomly assign every observation to either universe 1 or universe 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now tell Seaborn to create two plots in which the left plot only uses the data from **universe 1** and the right plot only uses the data from **universe 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col='universe' subsets the data by universe and creates two separate plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line looks pretty similar between the two plots, despite the fact that they used separate samples of data. In both cases, we would predict a brain weight of about 45 g.\n",
    "\n",
    "It's easier to see the degree of similarity by placing them on the same plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hue='universe' subsets the data by universe and creates a single plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** How would you characterize the bias and variance of a linear model for brain size against body size? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lets-try-something-completely-different\"></a>\n",
    "### Let's Try Something Completely Different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would a **low-bias, high-variance** model look like? Let's try polynomial regression with an eighth-order polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's **low bias** because the models match the data effectively.\n",
    "- It's **high variance** because the models are widely different, depending on which observations happen to be available in that universe. (For a body weight of 100 kg, the brain weight prediction would be 40 kg in one universe and 0 kg in the other!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"balancing-bias-and-variance\"></a>\n",
    "## Balancing Bias and Variance\n",
    "Can we find a middle ground?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we can create a model that has **less bias than the linear model** and **less variance than the eighth order polynomial**?\n",
    "\n",
    "Let's try a second order polynomial instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems better. In both the left and right plots, **it fits the data well, but not too well**.\n",
    "\n",
    "This is the essence of the **bias-variance trade-off**: You are seeking a model that appropriately balances bias and variance and thus will generalize to new data (known as \"out-of-sample\" data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "\n",
    "- Why do we care about variance if we know we can generate a more accurate model with higher complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Challenge:** Is it possible for a model that learns from the data to have zero bias and zero variance on a data set that has irreducible error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train-test-split\"></a>\n",
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation-procedure--train-and-test-on-the-entire-dataset-do-not-do-this\"></a>\n",
    "### Evaluation Procedure #1: Train and Test on the Entire Data Set (Do Not Do This)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train the model on the **entire data set**.\n",
    "2. Test the model on the **same data set** and evaluate how well we did by comparing the **predicted** response values with the **true** response values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Boston data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create X and y variables to store the feature matrix and response from the Boston data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame for both parts of data; don't forget to assign column names.\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target, columns=['MEDV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate y and X, then overwrite the Boston variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform basic EDA to make sure the data are in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values of each variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure data is numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at basic descriptive statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare a feature matrix (X) and response (y)  for scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import linear regression, instantiate, fit, and preview predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the predicted response values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a measure of model performance. The most common choices for regression problems are:\n",
    "\n",
    "- **R-squared**: The percentage of variance captured by the model.\n",
    "- **Mean squared error**: The average squared distance between the prediction and the correct answer.\n",
    "\n",
    "We'll use mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute mean squared error using a function from `metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is known as the **training mean squared error** because we are evaluating the model based on the same data we used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"problems-with-training-and-testing-on-the-same-data\"></a>\n",
    "### Problem With Training and Testing on the Same Data\n",
    "\n",
    "Maximizing the MSE on **in-sample data** rewards high-variance/low-bias models that fit the noise in the training data as well as the signal.\n",
    "\n",
    "Our goal is to estimate likely performance of a model on **out-of-sample data**, so a better approach is to put some data aside as a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation-procedure--traintest-split\"></a>\n",
    "### Evaluation procedure #2: Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split the data set into two pieces: a **training set** and a **testing set**.\n",
    "2. Train the model on the **training set**.\n",
    "3. Test the model on the **testing set** and evaluate how well we did.\n",
    "\n",
    "A common rule of thumb is to set aside 30% of your data set for testing.\n",
    "\n",
    "**Testing MSE is a better estimate of out-of-sample performance than training MSE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the `train_test_split` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train_test_split](./assets/train_test_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the `random_state` Parameter\n",
    "\n",
    "The `random_state` is a pseudo-random number that allows us to reproduce our results every time we run them. However, it makes it impossible to predict what are exact results will be if we chose a new `random_state`.\n",
    "\n",
    "`random_state` is very useful for testing that your model was made correctly since it provides you with the same split each time. However, make sure you remove it if you are testing for model variability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT a random_state parameter:\n",
    "#  (If you run this code several times, you get different results!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Print the first element of each object.\n",
    "print(X_train.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH a random_state parameter:\n",
    "#  (Same split every time! Note you can change the random state to any integer.)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Print the first element of each object.\n",
    "print(X_train.head(1))\n",
    "print(X_test.head(1))\n",
    "print(y_train.head(1))\n",
    "print(y_test.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Split X and y into training and testing sets (using `random_state` for reproducibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Test the model on the testing set and check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bias-variance tradeoff](./assets/bias_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go back to Step 1 and try adding new variables and transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Training error**: Decreases as model complexity increases (lower value of k).\n",
    "- **Testing error**: Is minimized at the optimum model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-test-performance-with-a-null-baseline\"></a>\n",
    "### Comparing Test Performance With a Null Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you know if your model is any good?\n",
    "\n",
    "It helps to have benchmarks to compare against. A very simple model can provide a *baseline* that you can use to check whether your model is complete garbage or not.\n",
    "\n",
    "A good baseline for linear regression is a model that just predicts the mean of the target variable every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .apply() to broadcast a mean for every prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** What does this result tell us about our linear regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-folds-cross-validation\"></a>\n",
    "## K-Folds Cross-Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple train/test split has at least two drawbacks:\n",
    "\n",
    "- Setting aside a test set that is large enough to provide a good estimate of performance can get in the way of training and lead you to choose a simpler model than you would need to do well when training on the full data set.\n",
    "- Evaluating many models on the same test set can lead to overfitting on the test set.\n",
    "\n",
    "**Solution:** Train the model on k separate train-test-splits, then average the resulting evaluation metric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/cross_validation_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach allows you to make each test set smaller while still getting a good estimate of accuracy and makes it harder to overfit to the test set by using all of the test set for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"leave-one-out-cross-validation\"></a>\n",
    "### Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extreme case of k-fold cross-validation is leave-one-out cross-validation, where you hold out just one data point at a time.\n",
    "\n",
    "This approach is great for making the best use of the training data while getting a good estimate of performance. However, it can be *very slow* because it requires fitting your model $n$ times.\n",
    "\n",
    "A more common approach is to set $k$ somewhere between 5 and 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro-to-cross-validation-with-the-boston-data\"></a>\n",
    "### Intro to Cross-Validation With the Boston Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a cross-validation with five folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"three-way-data-split\"></a>\n",
    "## Three-Way Data Split\n",
    "---\n",
    "Goodhart's law: \"When a measure becomes a target, it ceases to be a good measure.\"\n",
    "\n",
    "Cross-validation performance provides an upwardly biased estimate of out-of-sample performance when you use it to select a model, even with $k$-fold cross validation.\n",
    "\n",
    "If you care about being able to estimate model performance after tuning by cross-validation, then you should set aside a true test set from the beginning that you don't use even in cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/Train-Test-Split-CV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you care about having an unbiased estimate of model performance when you are done, then you should split your data three ways:\n",
    "    - **Training set**: A set of examples used for learning – what parameters of the classifier?\n",
    "    - **Validation set**: A set of examples used to tune the parameters of the classifier.\n",
    "    - **Testing set**: A set of examples used ONLY to assess the performance of the fully trained classifier.\n",
    "- The training-validation split can be k-fold.\n",
    "- Validation and testing must be separate data sets. Once you have the final model set, you cannot do any additional tuning after testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps for a three-way split**\n",
    "\n",
    "1. Set aside a testing set.\n",
    "2. Select architecture (model type) and training parameters ($k$).\n",
    "3. Train the model using the training set and evaluate it using the validation set (using either simple or k-fold train/test split).\n",
    "4. Repeat, selecting different architectures (models) and tuning parameters.\n",
    "5. Select the best model.\n",
    "6. Assess the model with the final testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"additional-resources\"></a>\n",
    "<a id=\"additional-resources\"></a>\n",
    "### Additional Resources\n",
    "- [Bias Variance](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "- University of Washington [slides](https://courses.cs.washington.edu/courses/cse546/12wi/slides/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "### Summary\n",
    "\n",
    "In this lab, we compared four methods of estimating model accuracy on out-of-sample data:\n",
    "\n",
    "1. **Train on the entire dataset**\n",
    "2. **Simple train/test split**\n",
    "3. **K-folds cross validation**\n",
    "4. **Three-way split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Exit Tickets](https://docs.google.com/forms/d/1BW4rVsCx8Nzp3q2B7SQ_tL1xqKZr4GGoQ5qeZfayxh4/viewform?ts=5ad40144&edit_requested=true)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
