{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n", " \n", "# Logistic Regression\n", " \n", "_Authors: Multiple_\n", " \n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"learning-objectives\"></a>\n", "### Learning Objectives\n", "\n", "By the end of this lesson, you will be able to..\n", "\n", "- Explain how logistic regression modifies linear regression for classification problems.\n", "- Interpret logistic regression coefficients.\n", "- Build a logistic regression model.\n", "- Calculate accuracy, true positive rate, and false negative rate from a confusion matrix.\n", "- Explain the limitations of accuracy as a classification metric.\n", "- Explain how to trade off true positive and false negative rate in a logistic regression model."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Intro\n", "\n", "Tonight we are talking about logistic regression, which despite its name is a classification algorithm."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise.**\n", "\n", "/poll \u201cWhich of the following are classification (as opposed to regression) problems? (Select all that apply.)\u201d \u201cPredicting how many people will come to a meetup event.\u201d \u201cPredicting which of the people who signed up for a meetup will actually attend.\u201d \u201cPredicting the price that a house will sell for, based on its zip code and square footage.\u201d \u201cAssigning probabilities of experiencing a fire in the next six months to buildings in a city.\u201d \u201cIdentifying animals in photographs by species.\u201d"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"refresher-fitting-and-visualizing-a-linear-regression-using-scikit-learn\"></a>\n", "## Refresher: Fitting and Visualizing a Linear Regression Using scikit-learn\n", "---\n", "\n", "Use Pandas to load in the glass attribute data from the UCI machine learning website. The columns are different measurements of properties of glass that can be used to identify the glass type. For detailed information on the columns in this data set, [please see the included .names file](http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.names)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["from pathlib import Path\n", "\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "import seaborn as sns\n", "\n", "%matplotlib inline\n", "sns.set(font_scale=1.5);"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["glass_filepath = Path('.', 'data', 'glass.csv')\n", "glass = pd.read_csv(glass_filepath)\n", "glass.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# change columns to something more uniform\n", "glass.columns = ['ri','na','mg','al','si','k','ca','ba','fe','glass_type']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Data Dictionary**\n", "\n", "- `Id`: number: 1 to 214\n", "- `RI`: refractive index  \n", "- `Na`: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n", "- `Mg`: Magnesium\n", "- `Al`: Aluminum\n", "- `Si`: Silicon\n", "- `K` : Potassium\n", "- `Ca`: Calcium\n", "- `Ba`: Barium\n", "- `Fe`: Iron\n", "- `Type` : Type of glass:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Let's build a regression model for refractice index against aluminum content.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#scatter with regression line\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Instantiate and fit a linear regression model predicting `ri` from `al` (and an intercept)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fit a linear regression model (name the model \"linreg\").\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Add a column `y_pred` to `glass` that stores the model's fitted values for the refractice index."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Make predictions for all values of X and add back to the original DataFrame.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Plot the predicted `ri` against each `al` as a line."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot those predictions connected by a line (try plt.plot()).\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Note the y axis labels when comparing to the scatterplot above._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Plot this regression line with the scatter points on the same chart."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Put the plots together (use a scatter and line graph).\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- How good would you say that this model is, based on the graph? Suggestion: think about how it compares to a \"null model\" that just predicts the mean reflective index regardless of the aluminum content."]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Print out the intercept and coefficient values from our fit `LinearRegression` object."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- What do these numbers mean?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Manually compute the predicted value of `ri` when `al=2.0` using the regression equation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compute prediction for al=2 using the equation.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Confirm that this is the same value we would get when using the built-in `.predict()` method of the `LinearRegression` object."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compute prediction for al=2 using the predict method.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"predicting-a-categorical-response\"></a>\n", "## Predicting a Single Categorical Response\n", "---\n", "\n", "Linear regression is appropriate when we want to predict the value of a continuous target/response variable, but what about when we want to predict membership in a class or category?\n", "\n", "**Examine the glass type column in the data set. What are the counts in each category?**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Examine glass_type.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Say these types are subdivisions of broader glass types:\n", "\n", "> **Window glass:** types 1, 2, and 3\n", "\n", "> **Household glass:** types 5, 6, and 7\n", "\n", "**Create a new `household` column that indicates whether or not a row is household glass, coded as 1 or 0, respectively.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Types 1, 2, 3 are window glass.\n", "# Types 5, 6, 7 are household glass.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's change our task, so that we're predicting the `household` category using `al`. Let's visualize the relationship to figure out how to do this.\n", "\n", "**Make a scatter plot comparing `al` and `household`.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**Fit a new `LinearRegression` predicting `household` from `al`.**\n", "\n", "Let's draw a regression line like we did before:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "scrolled": true}, "outputs": [], "source": ["# Fit a linear regression model and store the predictions.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# Scatter plot that includes the regression line\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If **al=3**, what class do we predict for household? **1**\n", "\n", "If **al=1.5**, what class do we predict for household? **0**\n", "\n", "We predict the 0 class for **lower** values of al, and the 1 class for **higher** values of al. What's our cutoff value? Around **al=2**, because that's where the linear regression line crosses the midpoint between predicting class 0 and class 1.\n", "\n", "Therefore, we'll say that if **household_pred >= 0.5**, we predict a class of **1**, else we predict a class of **0**."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Using this threshold, create a new column of our predictions for whether a row is household glass.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# np.where returns the first value if the condition is True,\n", "# and the second value if the condition is False.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# Transform household_pred to 1 or 0.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Plot a line that shows our predictions for class membership in household vs. not.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Linear regression yields a reasonable binary classifier in this case when we map values above 0.5 to 1 and values below 0.5 to 0.\n", "\n", "It would be nice if we could also interpret the raw numbers it gives us, e.g. as probabilities. The problem is that linear regression is unbounded. As a result, it gives values below 0 and above 1, which cannot be probabilities.\n", "\n", "This is where logistic regression comes in: it basically takes that linear regression line and bends its ends into an S-shape so that it always stays between 0 and 1, so that we can interpret its outputs as probabilities."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"using-logistic-regression-for-classification\"></a>\n", "## Using Logistic Regression for Classification\n", "---\n", "\n", "**Import the `LogisticRegression` class from `linear_model` below and fit the same regression model predicting `household` from `al`.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Fit a logistic regression model and store the class predictions.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Plot the predicted class using the logistic regression as we did for the linear regression predictions above.**\n", "\n", "As you can see, the class predictions are the same."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["# Plot the class predictions.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["What if we wanted the predicted probabilities instead of just the class predictions, to understand how confident we are in a given prediction?\n", "\n", "**Using the built-in `.predict_proba()` function, examine the predicted probabilities for the first handful of rows of `X`.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Sklearn orders the columns according to our class labels. The two-column output of `predict_proba` returns a column for each class of our `household` variable. The first column is the probability of `household=0` for a given row, and the second column is the probability of `household=1`.\n", "\n", "**Store the predicted probabilities of class=1 in its own column in the data set.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Store the predicted probabilities of class 1.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["glass.head(10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Plot the predicted probabilities as a line on our plot (probability of `household=1` as `al` changes).**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot the predicted probabilities.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Examine some example predictions.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise**\n", "\n", "- Build a logistic regression model for `household` using two features of your choice.\n", "- Do a simple train-test split on `glass`.\n", "- Train your model on the training set and evaluate it with `model.score` on the test set.\n", "\n", "**Bonus**\n", "\n", "Try out different sets of features to see which give the best results."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"probability-odds-ratio-e-log-and-log-odds\"></a>\n", "## Understanding Logistic Regression\n", "---\n", "\n", "**Recall:** A coefficient in a *linear regression* model tells you how the *number* predicted by the model changes when the associated variable increases by one and all other variables remain the same.\n", "\n", "**Similarly**, A coefficient in a *logistic regression* model tells you how the *log odds* predicted by the model changes when the associated variable increases by one and all other variables remain the same."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's try to develop some intuitions about log odds to help us reason about our logistic regression models."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Odds"]}, {"cell_type": "markdown", "metadata": {}, "source": ["$$probability = \\frac {one\\ outcome} {all\\ outcomes}$$\n", "\n", "$$odds = \\frac {one\\ outcome} {all\\ other\\ outcomes}$$\n", "\n", "It is often useful to think of the numeric odds as a ratio. For example, 5/1 = 5 odds is \"5 to 1\" -- five wins for every one loss (e.g. of six total plays). 2/3 odds means \"2 to 3\" -- two wins for every three losses (e.g. of five total plays).\n", "\n", "Examples:\n", "\n", "- Dice roll of 1: probability = 1/6, odds = 1/5\n", "- Even dice roll: probability = 3/6, odds = 3/3 = 1\n", "- Dice roll less than 5: probability = 4/6, odds = 4/2 = 2\n", "\n", "$$odds = \\frac {probability} {1 - probability}$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**As an example we can create a table of probabilities vs. odds, as seen below.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# Create a table of probability versus odds.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise.**\n", "\n", "Convert the following probabilities to odds:\n", "\n", "1. .25\n", "1. 1/3\n", "1. 2/3\n", "1. .95"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"understanding-e-and-the-natural-logarithm\"></a>\n", "### Understanding the Natural Logarithm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A logarithm tells you the *order of magnitude* of a number. The base-10 logarithm is a continuous version of \"the number of times you would need to multiply 10 to get that number.\"\n", "\n", "| number | number as a power of 10 | $\\log_{10}$(number) |\n", "| ------ | ----------------------------- |\n", "| $1 $|$ 10^0$ | 0 |\n", "| $10 $|$ 10^1$ | 1 |\n", "| $100 $|$ 10^2$ | 2 |\n", "| $1000 $|$ 10^3$ | 3 |\n", "\n", "It also works in the other direction:\n", "\n", "| number | number as a power of 10 | $\\log_{10}$(number) |\n", "| ------ | ----------------------------- |\n", "| $.001 $ | $ 10^{-3}$ | -3 |\n", "| $.01 $ | $ 10^{-2}$ | -2 |\n", "| $.1 $|$ 10^{-1}$ | -1 |\n", "| $1 $|$ 10^0$ | 0 |\n", "\n", "And for numbers in between exact powers of 10:\n", "\n", "| number | number as a power of 10 | $\\log_{10}$(number) |\n", "| ------ | ----------------------------- |\n", "| $1$ | $ 10^{0}$ | 0 |\n", "| $2$ | $ 10^{.301}$ | .301 |\n", "| $5$|$ 10^{.699}$ | .699 |\n", "| $10$|$ 10^1$ | 1 |\n", "| $20$|$ 10^{1.301}$ | 1.301 |\n", "| $50$|$ 10^{1.699}$ | 1.699 |\n", "| $100$|$ 10^2$ | 2 |"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Base $e$.** It is often convenient to use the special number $e$ as a base instead of 10. The interpretation is analogous: the base-$e$ logarithm of a number is a continuous version of \"the number of times you would have to multiple $e$ to get that number.\"\n", "\n", "For instance:\n", "\n", "| number | number as a power of $e$ | $\\log_{e}$(number) |\n", "| ------ | ----------------------------- |\n", "| $1 $|$ e^0$ | 0 |\n", "| $2.718$|$ e^1$ | 1 |\n", "| $7.39$|$ e^2$ | 2 |\n", "| $20.09$|$ e^3$ | 3 |\n", "\n", "It also works in the other direction:\n", "\n", "| number | number as a power of $e$ | $\\log_{e}$(number) |\n", "| ------ | ----------------------------- |\n", "| $.050 $ | $ e^{-3}$ | -3 |\n", "| $.135 $ | $ e^{-2}$ | -2 |\n", "| $.368 $|$ e^{-1}$ | -1 |\n", "| $1 $|$ e^0$ | 0 |\n", "\n", "And for numbers in between exact powers of $e$:\n", "\n", "| number | number as a power of $e$ | $\\log_{e}$(number) |\n", "| ------ | ----------------------------- |\n", "| $1$ | $ e^{0}$ | 0 |\n", "| $1.35$ | $ e^{.301}$ | .301 |\n", "| $2.01$|$ e^{.699}$ | .699 |\n", "| $2.718$|$ e^1$ | 1 |\n", "| $3.67$|$ e^{1.301}$ | 1.301 |\n", "| $5.47$|$ e^{1.699}$ | 1.699 |\n", "| $7.39$|$ e^2$ | 2 |"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When we take the **logarithm** of an **odds** we get the **log odds**.\n", "\n", "The most common convention is to use base-$e$ logarithms unless otherwise specified."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add log odds to the table.\n", "table['logodds'] = np.log(table['odds'])\n", "table"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Notice:** log odds goes to $-\\infty$ as probability goes to 0, and goes to $\\infty$ as probability goes to 1.\n", "\n", "**Consequence:** The fact that linear model is unbounded is fine if we use it to model *log odds* rather than *probability*."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"what-is-logistic-regression\"></a>\n", "### What Is Logistic Regression?\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Linear regression:** *Continuous response* is modeled as a linear combination of the features.\n", "\n", "$$y = \\beta_0 + \\beta_1x$$\n", "\n", "**Logistic regression:** *Log odds* is modeled as a linear combination of the features.\n", "\n", "$$\\log \\left({p\\over 1-p}\\right) = \\beta_0 + \\beta_1x$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This equation can be rearranged to get the predicted probability:\n", "\n", "$$\\hat{p} = \\frac{e^{\\beta_0 + \\beta_1x}} {1 + e^{\\beta_0 + \\beta_1x}}$$\n", "\n", "This equation gives us the \"S\" (sigmoid) shape for the predicted probability as a function of $\\beta_1$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### How do we interpret the regression parameters?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Linear regression:**\n", "\n", "$$y = \\beta_0 + \\beta_1x$$\n", "\n", "- $\\beta_0$ tells you the model's prediction for $y$ when all input features are zero.\n", "- $\\beta_1$ tells you how the model's prediction for $y$ changes with a one-unit increase in $x$ when all other variables remain the same.\n", "\n", "**Logistic regression:**\n", "\n", "$$\\log \\left({p\\over 1-p}\\right) = \\beta_0 + \\beta_1x$$\n", "\n", "- $\\beta_0$ tells you the model's prediction for the *log odds of $y$* when all input features are zero.\n", "- $\\beta_1$ tells you how the model's prediction for *the log odds of* $y$ changes with a one-unit increase in $x$ when all other variables remain the same."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Bottom line:** A positive coefficient means that the predicted log odds of the response (and thus the predicted probability) increases with the associated variable, while a negative coefficient means that it decreases."]}, {"cell_type": "markdown", "metadata": {}, "source": ["![Logistic regression beta values](./assets/logistic_betas.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Changing the $\\beta_0$ value shifts the curve horizontally, whereas changing the $\\beta_1$ value changes the slope of the curve."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Summary\n", "\n", "- Logistic regression addresses a binary classification problem by modeling the *log odds* that an individual is in the class as a linear function of the model features.\n", "- A coefficient in a logistic regression model tells you *how the log odds that the model predicts changes* with a one-unit increase in the associated input feature, while other features remain unchanged.\n", "- The model's log-odds predictions can be transformed into *probabilities*.\n", "- Those predicted probabilities follow an \"s\" (sigmoid) shape that is bounded by 0 and 1, as a function of the input features.\n", "- Those predicted probabilities can be converted into \"hard\" class predictions by mapping everything above a threshold to 1 and everything below it to 0."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"comparing-logistic-regression-to-other-models\"></a>\n", "## Comparing Logistic Regression to Other Models\n", "---\n", "\n", "Advantages of logistic regression:\n", "\n", "- Somewhat interpretable.\n", "- Training and prediction are fast.\n", "- Outputs probabilities.\n", "- Features don't need scaling.\n", "- Can perform well with a small number of observations.\n", "\n", "Disadvantages of logistic regression:\n", "\n", "- Presumes a linear relationship between the features and the log odds of the response.\n", "- Performance is (generally) not competitive with the best supervised learning methods.\n", "- Can't automatically learn feature interactions."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"advanced-classification-metrics\"></a>\n", "## Advanced Classification Metrics\n", "\n", "---\n", "\n", "By default, the `.score` method of a logistic regression model in sklearn returns accuracy:\n", "\n", "$$Accuracy = \\frac{total~predicted~correct}{total~predicted}$$\n", "\n", "However, accuracy is not always the most relevant metric."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Consider the **confusion matrix** for a binary classification problem where we have 165 observations/rows of people who are either smokers or nonsmokers.\n", "\n", "<table style=\"border: none\">\n", "<tr style=\"border: none\">\n", "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n", "    <td style=\"\"><b>Predicted: No</b></td>\n", "    <td style=\"\"><b>Predicted: Yes</b></td>\n", "</tr>\n", "<tr>\n", "    <td><b>Actual: No</b></td>\n", "    <td style=\"text-align: center\"><font color=\"blue\">TN = 50</font></td>\n", "    <td style=\"text-align: center\"><font color=\"red\">FP = 10</font></td>\n", "    <td style=\"text-align: center\">60</td>\n", "</tr>\n", "<tr>\n", "    <td><b>Actual: Yes</b></td>\n", "    <td style=\"text-align: center\"><font color=\"orange\">FN = 5</font></td>\n", "    <td style=\"text-align: center\"><font color=\"green\">TP = 100</font></td>\n", "    <td style=\"text-align: center\">105</td>\n", "</tr>\n", "<tr style=\"border: none\">\n", "    <td style=\"border: none\"></td>\n", "    <td style=\"text-align: center\">55</td>\n", "    <td style=\"text-align: center\">110</td>\n", "</tr>\n", "\n", "</table>\n", "\n", "\n", "- <font color=\"green\">**True positives (TP):**</font> These are cases in which we predicted yes (smokers), and they actually are smokers.\n", "- <font color=\"blue\">**True negatives (TN):**</font> We predicted no, and they are nonsmokers.\n", "- <font color=\"red\">**False positives (FP):**</font> We predicted yes, but they were not actually smokers. (This is also known as a \"Type I error.\")\n", "- <font color=\"orange\">**False negatives (FN):**</font> We predicted no, but they are smokers. (This is also known as a \"Type II error.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise.**\n", "\n", "Categorize these cases as TP, TN, FP, or FN.\n", "    \n", "- We predict that a growth is malignant, and it is benign. (is_malignant=1)\n", "- We predict that an image does not contain a cat, and it does not. (has_cat=1)\n", "- We predict that a locomotive will fail in the next two weeks, and it does. (breaks=1)\n", "- We predict that a user will like a song, and she does not. (likes_song=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"accuracy-true-positive-rate-and-false-negative-rate\"></a>\n", "### Accuracy, True Positive Rate, and False Negative Rate"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Accuracy:** Overall, how often is the classifier correct?\n", "\n", "<span>\n", "    (<span style=\"color: green\">TP</span>+<span style=\"color: blue\">TN</span>)/<span style=\"color: purple\">total</span> = (<span style=\"color: green\">100</span>+<span style=\"color: blue\">50</span>)/<span style=\"color: purple\">165</span> = 0.91\n", "</span>\n", "\n", "<table style=\"border: none\">\n", "<tr style=\"border: none\">\n", "    <td style=\"border: none; vertical-align: bottom; color: purple\">n = 165</td>\n", "    <td style=\"\"><b>Predicted: No</b></td>\n", "    <td style=\"\"><b>Predicted: Yes</b></td>\n", "</tr>\n", "<tr>\n", "    <td><b>Actual: No</b></td>\n", "    <td style=\"text-align: center; background-color: blue\">TN = 50</td>\n", "    <td style=\"text-align: center\">FP = 10</td>\n", "    <td style=\"text-align: center\">60</td>\n", "</tr>\n", "<tr>\n", "    <td><b>Actual: Yes</b></td>\n", "    <td style=\"text-align: center\">FN = 5</td>\n", "    <td style=\"text-align: center; background-color: green\">TP = 100</td>\n", "    <td style=\"text-align: center\">105</td>\n", "</tr>\n", "<tr style=\"border: none\">\n", "    <td style=\"border: none\"></td>\n", "    <td style=\"text-align: center\">55</td>\n", "    <td style=\"text-align: center\">110</td>\n", "</tr>\n", "\n", "</table>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**True positive rate (TPR)** asks, \u201cOut of all of the target class labels, how many were accurately predicted to belong to that class?\u201d\n", "\n", "For example, given a medical exam that tests for cancer, how often does it correctly identify patients with cancer?\n", "\n", "<span>\n", "<span style=\"color: green\">TP</span>/<span style=\"color: aqua\">actual yes</span> = <span style=\"color: green\">100</span>/<span style=\"color: aqua\">105</span> = 0.95\n", "</span>\n", "\n", "<table style=\"border: none\">\n", "<tr style=\"border: none\">\n", "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n", "    <td style=\"\"><b>Predicted: No</b></td>\n", "    <td style=\"\"><b>Predicted: Yes</b></td>\n", "</tr>\n", "<tr>\n", "    <td><b>Actual: No</b></td>\n", "    <td style=\"text-align: center\">TN = 50</td>\n", "    <td style=\"text-align: center\">FP = 10</td>\n", "    <td style=\"text-align: center\">60</td>\n", "</tr>\n", "<tr>\n", "    <td><b>Actual: Yes</b></td>\n", "    <td style=\"text-align: center\">FN = 5</td>\n", "    <td style=\"text-align: center;background-color: green\">TP = 100</td>\n", "    <td style=\"text-align: center;color: aqua\">105</td>\n", "</tr>\n", "<tr style=\"border: none\">\n", "    <td style=\"border: none\"></td>\n", "    <td style=\"text-align: center\">55</td>\n", "    <td style=\"text-align: center\">110</td>\n", "</tr>\n", "\n", "</table>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**False positive rate (FPR)** asks, \u201cOut of all items not belonging to a class label, how many were predicted as belonging to that target class label?\u201d\n", "\n", "For example, given a medical exam that tests for cancer, how often does it trigger a \u201cfalse alarm\u201d by incorrectly saying a patient has cancer?\n", "\n", "<span>\n", "<span style=\"color: orange\">FP</span>/<span style=\"color: fuchsia\">actual no</span> = <span style=\"color: orange\">10</span>/<span style=\"color: fuchsia\">60</span> = 0.17\n", "</span>\n", "\n", "<table style=\"border: none\">\n", "<tr style=\"border: none\">\n", "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n", "    <td style=\"\"><b>Predicted: No</b></td>\n", "    <td style=\"\"><b>Predicted: Yes</b></td>\n", "</tr>\n", "<tr>\n", "    <td><b>Actual: No</b></td>\n", "    <td style=\"text-align: center\">TN = 50</td>\n", "    <td style=\"text-align: center;background-color: orange\">FP = 10</td>\n", "    <td style=\"text-align: center;color:fuchsia\">60</td>\n", "</tr>\n", "<tr>\n", "    <td><b>Actual: Yes</b></td>\n", "    <td style=\"text-align: center\">FN = 5</td>\n", "    <td style=\"text-align: center\">TP = 100</td>\n", "    <td style=\"text-align: center\">105</td>\n", "</tr>\n", "<tr style=\"border: none\">\n", "    <td style=\"border: none\"></td>\n", "    <td style=\"text-align: center\">55</td>\n", "    <td style=\"text-align: center\">110</td>\n", "</tr>\n", "\n", "</table>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise.**\n", "\n", "We turn the probabilities output by a logistic regression model into \"hard\" predictions by setting a threshold. For instance, we might treat all probabilities above .5 as positive predictions and the rest as negative predictions.\n", "\n", "- How does the true positive rate of a logistic regression model change if we change the threshold probability for treating a prediction as positive from .5 to .6?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- How does the false positive rate of a logistic regression model change if we change the threshold probability for treating a prediction as positive from .5 to .6?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Describe a situation in which you would want to use a high threshold probability."]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Describe a situation in which you would want to use a low threshold probability."]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Calculate the accuracy, true positive rate, and false positive rate for the confusion matrix below.\n", "\n", "<table style=\"border: none\">\n", "<tr style=\"border: none\">\n", "    <td style=\"border: none; vertical-align: bottom\">n = 140</td>\n", "    <td style=\"\"><b>Predicted: No</b></td>\n", "    <td style=\"\"><b>Predicted: Yes</b></td>\n", "</tr>\n", "<tr>\n", "    <td><b>Actual: No</b></td>\n", "    <td style=\"text-align: center\">30</td>\n", "    <td style=\"text-align: center\">10</td>\n", "    <td style=\"text-align: center\">40</td>\n", "</tr>\n", "<tr>\n", "    <td><b>Actual: Yes</b></td>\n", "    <td style=\"text-align: center\">60</td>\n", "    <td style=\"text-align: center\">40</td>\n", "    <td style=\"text-align: center\">100</td>\n", "</tr>\n", "<tr style=\"border: none\">\n", "    <td style=\"border: none\"></td>\n", "    <td style=\"text-align: center\">90</td>\n", "    <td style=\"text-align: center\">50</td>\n", "</tr>\n", "\n", "</table>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Example\n", "\n", "The true positive and false positive rates gives us a much clearer picture of where predictions begin to fall apart."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["from sklearn import linear_model, metrics, model_selection\n", "\n", "admissions_path = Path('.', 'data', 'admissions.csv')\n", "admissions = pd.read_csv(admissions_path).dropna()\n", "admissions.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Split data, train model\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Score model\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compare to null model\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Get the confusion matrix\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise**\n", "\n", "- What is our model doing?\n", "- What is the model's accuracy on the test set?\n", "- What is the model's true positive rate?\n", "- What is the model's false positive rate?"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**We can vary the classification threshold for our model to get different predictions.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["**Exercise**\n", "\n", "- What is the model's accuracy on the test set?\n", "- What is the model's true positive rate?\n", "- What is the model's false positive rate?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Accuracy\n", "\n", "**Advantages:**\n", "\n", "- Intuitive: it's a lot like an exam score where you get total correct/total attempted.\n", "\n", "**Disadvantages:**\n", "\n", "- Potentially misleading: Can look OK when model is just outputting the most common label.\n", "    - Particularly bad when classes are imbalanced -- e.g. train doesn't break 99% of the time, so a model that always says \"won't break\" has 99% accuracy -- but it fails exactly when we need it!\n", "- Doesn't account for relative costs of false positives and false negatives.\n", "- Doesn't say anything about how far predicted probabilities are from the correct labels."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Other metrics to investigate:**\n", "    \n", "- **Classification error:** Proportion of incorrect predictions (1-accuracy, lower is better).\n", "- **Receiver Operating Characteristic (ROC) curves:** True positive rate vs. false positive rate across all possible threshold probabilities. The **area under the ROC curve** (AUC) is a measure of how well your model performs overall across those thresholds.\n", "  - Allows you to visualize the performance of your classifier across all possible classification thresholds, thus helping you to choose a threshold that appropriately balances true positives and false positives.\n", "  - Still useful when there is high class imbalance (unlike classification accuracy/error).\n", "  - Harder to use when there are more than two response classes.\n", "- **Log loss**: Measures how far the output probabilities are from the correct labels. (Useful when you want to make expected value calculations with those probabilities or triage cases for further attention.)\n", "- **True Negative Rate**, **False Negative Rate**\n", "- **Recall** (a.k.a. True Positive Rate), **Precision** (proportion of positive predictions that are true)\n", "\n", "These measures are all readily available in sklearn."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Lesson Review\n", "- **Logistic regression**\n", "  - What kind of machine learning problems does logistic regression address?\n", "  - What do the coefficients in a logistic regression represent? How does the interpretation differ from ordinary least squares? How is it similar?\n", "  \n", "- **The confusion matrix**\n", "  - Why isn't accuracy all you need to evaluate classification models?\n", "  - How can you tune a model based on the relative costs of false positives and false negatives?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Projects\n", "\n", "- Final Project Pt 2 due today\n", "- Final Project Pt 3 due Thurs."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Questions?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["```\n", "=========================================\n", "@channel\n", "Exit Ticket: https://goo.gl/forms/OUw4gyTiRKMOTI3t2        \n", "\n", "#feedback\n", "=========================================\n", "```"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.3"}}, "nbformat": 4, "nbformat_minor": 1}